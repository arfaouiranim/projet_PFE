# -*- coding: utf-8 -*-
"""ML stage PFE regression logistique .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fFnTojKM0f2g_2eMsMtJ4HOys5fodPRv
"""

from google.colab import drive
# Monter Google Drive pour acc√©der aux fichiers
drive.mount('/content/drive')

"""**regression logistique**"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef, roc_curve, auc
import numpy as np
from google.colab import drive

# Monter Google Drive
drive.mount('/content/drive')

# Chemins des fichiers CSV
train_file_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE train_file.csv"
test_file_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE test_file.csv"

# Charger les donn√©es
train_df = pd.read_csv(train_file_path)
test_df = pd.read_csv(test_file_path)

# S√©lectionner les features et la variable cible
features_to_exclude = ['code de miRNA', 'nombre of species', 's√©quence de miRNA', 'esp√®ce', 'label']
X_train = train_df.drop(columns=features_to_exclude, errors='ignore')
y_train = train_df['label']
X_test = test_df.drop(columns=features_to_exclude, errors='ignore')
y_test = test_df['label']

# Cr√©ation et entra√Ænement du mod√®le
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train, y_train)

# Pr√©dictions
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]  # Probabilit√© de la classe positive

# √âvaluation du mod√®le
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Calcul de la courbe ROC et de l'AUC
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Affichage des r√©sultats
def print_metrics():
    print("√âvaluation du mod√®le de r√©gression logistique :")
    print(f"Exactitude (Accuracy) : {accuracy:.4f}")
    print(f"Rappel (Recall) : {recall:.4f}")
    print(f"Score F1 : {f1:.4f}")
    print(f"MCC : {mcc:.4f}")
    print(f"AUC : {roc_auc:.4f}")
    print("Matrice de confusion :")
    print(conf_matrix)

print_metrics()

# Sauvegarde des r√©sultats dans un fichier texte
results_path = "/content/drive/MyDrive/stage PFE logistic_regression_results.txt"
with open(results_path, "w") as f:
    f.write("√âvaluation du mod√®le de r√©gression logistique :\n")
    f.write(f"Exactitude (Accuracy) : {accuracy:.4f}\n")
    f.write(f"Rappel (Recall) : {recall:.4f}\n")
    f.write(f"Score F1 : {f1:.4f}\n")
    f.write(f"MCC : {mcc:.4f}\n")
    f.write(f"AUC : {roc_auc:.4f}\n")
    f.write("Matrice de confusion :\n")
    f.write(np.array2string(conf_matrix))

print(f"R√©sultats enregistr√©s dans {results_path}")

# Tracer et enregistrer la courbe ROC
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Taux de Faux Positifs (FPR)')
plt.ylabel('Taux de Vrais Positifs (TPR)')
plt.title('Courbe ROC')
plt.legend(loc='lower right')
roc_curve_path = "/content/drive/MyDrive/stage PFE logistic_regression_roc_curve.png"
plt.savefig(roc_curve_path)
plt.show()

print(f"Courbe ROC enregistr√©e dans {roc_curve_path}")

# Optionnel: Affichage des probabilit√©s pour quelques √©chantillons
print("\nExemple de probabilit√©s pr√©dites (5 premiers √©chantillons):")
for i in range(5):
    print(f"√âchantillon {i+1}: Probabilit√© = {y_prob[i]:.4f} => Pr√©diction = {y_pred_threshold[i]} (R√©el = {y_test.iloc[i]})")

"""**Regressio logistique + LASSO + R√©sultat de LASSO en csv**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef, roc_curve, auc
from sklearn.preprocessing import StandardScaler

# Monter Google Drive
drive.mount('/content/drive')

# Chemins des fichiers CSV
train_file_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE train_file.csv"
test_file_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE test_file.csv"

# Charger les donn√©es
train_df = pd.read_csv(train_file_path)
test_df = pd.read_csv(test_file_path)

# S√©lectionner les features et la variable cible
features_to_exclude = ['code de miRNA', 'nombre of species', 's√©quence de miRNA', 'esp√®ce', 'label']
X_train = train_df.drop(columns=features_to_exclude, errors='ignore')
y_train = train_df['label']
X_test = test_df.drop(columns=features_to_exclude, errors='ignore')
y_test = test_df['label']

# Normalisation des donn√©es
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Cr√©ation et entra√Ænement du mod√®le avec r√©gularisation L1 (Lasso)
model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=42)
model.fit(X_train_scaled, y_train)

# Extraction du param√®tre lambda (Œª = 1/C)
lambda_value = 1 / model.C
print(f"Valeur du param√®tre de r√©gularisation lambda: {lambda_value:.4f}")

# Pr√©dictions
y_pred = model.predict(X_test_scaled)
y_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probabilit√© de la classe positive

# √âvaluation du mod√®le
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Calcul de la courbe ROC et de l'AUC
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Affichage des r√©sultats
def print_metrics():
    print("√âvaluation du mod√®le de r√©gression logistique avec Lasso :")
    print(f"Param√®tre de r√©gularisation (lambda) : {lambda_value:.4f}")  # Nouvelle ligne
    print(f"Exactitude (Accuracy) : {accuracy:.4f}")
    print(f"Rappel (Recall) : {recall:.4f}")
    print(f"Score F1 : {f1:.4f}")
    print(f"MCC : {mcc:.4f}")
    print(f"AUC : {roc_auc:.4f}")
    print("Matrice de confusion :")
    print(conf_matrix)

print_metrics()

# Sauvegarde des r√©sultats dans un fichier texte
results_path = "/content/drive/MyDrive/stage PFE logistic_regression_lasso_results.txt"
with open(results_path, "w") as f:
    f.write("√âvaluation du mod√®le de r√©gression logistique avec Lasso :\n")
    f.write(f"Param√®tre de r√©gularisation (lambda) : {lambda_value:.4f}\n")  # Nouvelle ligne
    f.write(f"Exactitude (Accuracy) : {accuracy:.4f}\n")
    f.write(f"Rappel (Recall) : {recall:.4f}\n")
    f.write(f"Score F1 : {f1:.4f}\n")
    f.write(f"MCC : {mcc:.4f}\n")
    f.write(f"AUC : {roc_auc:.4f}\n")
    f.write("Matrice de confusion :\n")
    f.write(np.array2string(conf_matrix))

print(f"R√©sultats enregistr√©s dans {results_path}")

# Tracer et enregistrer la courbe ROC
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Taux de Faux Positifs (FPR)')
plt.ylabel('Taux de Vrais Positifs (TPR)')
plt.title('Courbe ROC')
plt.legend(loc='lower right')
roc_curve_path = "/content/drive/MyDrive/stage PFE logistic_regression_lasso_roc_curve.png"
plt.savefig(roc_curve_path)
plt.show()

print(f"Courbe ROC enregistr√©e dans {roc_curve_path}")

# Extraction des coefficients Lasso
coefficients = model.coef_[0]
feature_names = X_train.columns
lasso_coeffs = pd.DataFrame({"Feature": feature_names, "Coefficient": coefficients})

# Ajouter une colonne indiquant si le coefficient est supprim√© ou retenu
lasso_coeffs['Retained'] = lasso_coeffs['Coefficient'].apply(lambda x: 'Retained' if x != 0 else 'Zeroed')

# Sauvegarde des coefficients dans un fichier CSV
output_coeffs_path = "/content/drive/MyDrive/stage PFE lasso_coefficients.csv"
lasso_coeffs.to_csv(output_coeffs_path, index=False)
print(f"Coefficients de Lasso sauvegard√©s dans : {output_coeffs_path}")

"""**regression logistique + LASSO + GridSearchCV**

"""

import pandas as pd
import numpy as np
from google.colab import drive
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef, roc_curve, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt

# Monter Google Drive
drive.mount('/content/drive')

# Chemins des fichiers train et test
train_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE train_file.csv"
test_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE test_file.csv"

# Charger les fichiers CSV
df_train = pd.read_csv(train_path)
df_test = pd.read_csv(test_path)

# Supprimer la colonne 'nombre of species'
df_train = df_train.drop(columns=['nombre of species'], errors='ignore')
df_test = df_test.drop(columns=['nombre of species'], errors='ignore')

X_train = df_train.drop(columns=['label', 'code de miRNA', 's√©quence de miRNA', 'esp√®ce'])
y_train = df_train['label']
X_test = df_test.drop(columns=['label', 'code de miRNA', 's√©quence de miRNA', 'esp√®ce'])
y_test = df_test['label']

# Normaliser les donn√©es
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# D√©finir les valeurs de C √† tester (C = 1/ùúÜ)
C_values = [0.01, 0.1, 1, 100, 1000, 3000]

# GridSearchCV pour trouver le meilleur C
lasso = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)
grid = GridSearchCV(lasso, param_grid={'C': C_values}, cv=5, scoring='accuracy')
grid.fit(X_train_scaled, y_train)

# Meilleur C trouv√©
best_C = grid.best_params_['C']
best_lambda = 1 / best_C  # Conversion en lambda
print(f"Meilleur C trouv√© : {best_C}")
print(f"Valeur correspondante de Œª (lambda) : {best_lambda:.4f}")

# Entra√Æner le mod√®le avec le meilleur C
model = LogisticRegression(penalty='l1', solver='liblinear', C=best_C, random_state=42)
model.fit(X_train_scaled, y_train)

# Pr√©dictions avec seuil de 0.5
y_pred = model.predict(X_test_scaled)
y_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probabilit√©s de la classe positive

# Calcul du catof (taux de classification) avec seuil √† 0.5
threshold = 0.5
y_pred_threshold = (y_prob >= threshold).astype(int)
catof = accuracy_score(y_test, y_pred_threshold)

# √âvaluation du mod√®le
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])

# Afficher les r√©sultats
print("\n" + "="*50)
print("R√©sultats avec seuil par d√©faut (0.5):")
print(f"Accuracy (CatOF): {catof:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"Specificity: {specificity:.4f}")
print(f"MCC: {mcc:.4f}")
print("Confusion Matrix:")
print(conf_matrix)

# Sauvegarder les r√©sultats
results_path = "/content/drive/MyDrive/stage PFE lasso_results.txt"
with open(results_path, "w") as file:
    file.write(f"Meilleur C trouv√© : {best_C}\n")
    file.write(f"Valeur de lambda (ùúÜ) : {best_lambda:.4f}\n")
    file.write("\nR√©sultats avec seuil √† 0.5:\n")
    file.write(f"Accuracy (CatOF): {catof:.4f}\n")
    file.write(f"Recall: {recall:.4f}\n")
    file.write(f"F1-score: {f1:.4f}\n")
    file.write(f"Specificity: {specificity:.4f}\n")
    file.write(f"MCC: {mcc:.4f}\n")
    file.write("Confusion Matrix:\n")
    file.write(np.array2string(conf_matrix))

print(f"R√©sultats enregistr√©s dans : {results_path}")

# Obtenir les coefficients de Lasso
coefficients = model.coef_[0]
features = X_train.columns
lasso_results = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})

# Ajouter une colonne indiquant si le coefficient est supprim√© ou retenu
lasso_results['Retained'] = lasso_results['Coefficient'].apply(lambda x: 'Retained' if x != 0 else 'Zeroed')

# Sauvegarder tous les coefficients dans un seul fichier CSV
coefficients_path = "/content/drive/MyDrive/stage PFE lasso_coefficients.csv"
lasso_results.to_csv(coefficients_path, index=False)
print(f"Coefficients sauvegard√©s dans : {coefficients_path}")

# Calculer la courbe ROC
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)

# Afficher la courbe ROC
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'Courbe ROC (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('Taux de faux positifs (FPR)')
plt.ylabel('Taux de vrais positifs (TPR)')
plt.title('Courbe ROC')
plt.legend(loc='lower right')
plt.grid(True)

# Sauvegarder la courbe ROC dans le Drive
roc_curve_path = "/content/drive/MyDrive/stage PFE Lasso_courbe_ROC.png"
plt.savefig(roc_curve_path)
print(f"Courbe ROC sauvegard√©e dans : {roc_curve_path}")

# Affichage dans le notebook
plt.show()

# Afficher AUC
print(f"AUC (Area Under the Curve): {roc_auc:.4f}")

# Optionnel: Affichage des probabilit√©s pour quelques √©chantillons
print("\nExemple de probabilit√©s pr√©dites (5 premiers √©chantillons):")
for i in range(5):
    print(f"√âchantillon {i+1}: Probabilit√© = {y_prob[i]:.4f} => Pr√©diction = {y_pred_threshold[i]} (R√©el = {y_test.iloc[i]})")

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
import joblib
import matplotlib.pyplot as plt

# === Chemins ===
new_data_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE selected_non_conserved_miRNAs3.csv"
output_predictions_path = "/content/drive/MyDrive/stage PFE lasso_predictions_on_new_data.csv"

# === Charger le nouveau jeu de donn√©es ===
new_df = pd.read_csv(new_data_path)

# Sauvegarder les identifiants (si dispo)
if 'code de miRNA' in new_df.columns:
    mirna_ids = new_df['code de miRNA']
else:
    mirna_ids = pd.Series(np.arange(len(new_df)), name='code de miRNA')

# Supprimer les colonnes non utilis√©es
X_new = new_df.drop(columns=['code de miRNA', 's√©quence de miRNA', 'esp√®ce', 'nombre of species'], errors='ignore')

# === G√©rer les NaN : remplacer par la moyenne ===
imputer = SimpleImputer(strategy='mean')
X_new_imputed = imputer.fit_transform(X_new)

# === Normaliser avec le scaler d√©j√† entra√Æn√© ===
X_new_scaled = scaler.transform(X_new_imputed)

# === Pr√©dire avec le mod√®le ===
y_new_prob = model.predict_proba(X_new_scaled)[:, 1]
y_new_pred = (y_new_prob >= 0.5).astype(int)

# === R√©sultats dans un DataFrame ===
results_df = pd.DataFrame({
    'code de miRNA': mirna_ids,
    'Probabilit√© classe 1': y_new_prob,
    'Pr√©diction': y_new_pred
})

# === Sauvegarder les r√©sultats ===
results_df.to_csv(output_predictions_path, index=False)
print(f"\n Pr√©dictions sauvegard√©es dans : {output_predictions_path}\n")

# === Affichage des r√©sultats dans Colab ===

# Afficher les 5 premi√®res pr√©dictions
print(" Aper√ßu des 5 premi√®res pr√©dictions :")
display(results_df.head())

# Statistiques de r√©partition
print("\n R√©partition des classes pr√©dites :")
print(results_df['Pr√©diction'].value_counts())

# Afficher les 10 miRNAs avec la plus haute probabilit√© d'√™tre positifs
print("\n Top 10 des miRNAs les plus probables (classe 1) :")
display(results_df.sort_values(by='Probabilit√© classe 1', ascending=False).head(10))

# Histogramme des probabilit√©s
plt.figure(figsize=(8, 5))
plt.hist(y_new_prob, bins=30, color='skyblue', edgecolor='black')
plt.title("Distribution des probabilit√©s pr√©dites (classe 1)")
plt.xlabel("Probabilit√©")
plt.ylabel("Nombre de miRNAs")
plt.grid(True)
plt.show()