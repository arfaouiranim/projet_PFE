# -*- coding: utf-8 -*-
"""decision_tree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gT7cJchyWPUmnFkLgbN5Hqx7-1DQxQbF
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV

# Monter Google Drive
drive.mount('/content/drive')

"""**Decision Tree**

cette cellule l'arbre de decision est sans limit de profondeur puis on applique un pruning
"""

# Chemins des fichiers train et test
train_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE train_file.csv"
test_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE test_file.csv"

# Charger les fichiers CSV
df_train = pd.read_csv(train_path)
df_test = pd.read_csv(test_path)

# Supprimer la colonne 'nombre of species'
df_train = df_train.drop(columns=['nombre of species'], errors='ignore')
df_test = df_test.drop(columns=['nombre of species'], errors='ignore')

# Séparer les features et les labels
X_train = df_train.drop(columns=['label', 'code de miRNA', 'séquence de miRNA', 'espèce'])
y_train = df_train['label']
X_test = df_test.drop(columns=['label', 'code de miRNA', 'séquence de miRNA', 'espèce'])
y_test = df_test['label']

# Normaliser les données
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Définir les hyperparamètres à tester
param_grid = {
    'min_samples_split': [ 5],
    'min_samples_leaf': [2]
}

# GridSearchCV pour trouver les meilleurs hyperparamètres
dt = DecisionTreeClassifier(random_state=42)
grid = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')
grid.fit(X_train_scaled, y_train)

# Meilleurs hyperparamètres trouvés
best_params = grid.best_params_
print(f"Meilleurs hyperparamètres trouvés : {best_params}")

# Entraîner le modèle sans limite de profondeur
model = DecisionTreeClassifier(**best_params, random_state=42)
model.fit(X_train_scaled, y_train)

# Élagage basé sur la complexité de coût minimal (ccp_alpha)
path = model.cost_complexity_pruning_path(X_train_scaled, y_train)
ccp_alphas = path.ccp_alphas[:-1]  # On ignore le dernier alpha qui donne un arbre vide

# Tester différents alpha pour l'élagage
pruned_models = [DecisionTreeClassifier(**best_params, ccp_alpha=alpha, random_state=42).fit(X_train_scaled, y_train) for alpha in ccp_alphas]

# Sélectionner le meilleur alpha selon l'AUC
aoc_scores = [auc(*roc_curve(y_test, model.predict_proba(X_test_scaled)[:, 1])[:2]) for model in pruned_models]
best_alpha = ccp_alphas[np.argmax(aoc_scores)]

# Réentraîner avec le meilleur alpha
pruned_model = DecisionTreeClassifier(**best_params, ccp_alpha=best_alpha, random_state=42)
pruned_model.fit(X_train_scaled, y_train)

# Prédictions
y_pred = pruned_model.predict(X_test_scaled)
y_proba = pruned_model.predict_proba(X_test_scaled)[:, 1]

# Calcul de l'AUC et courbe ROC
fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

# Tracer la courbe ROC
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Courbe ROC - Decision Tree (élagage)')
plt.legend(loc='lower right')
roc_curve_path = "/content/drive/MyDrive/stage PFE decision_tree_roc_curve_pruned.png"
plt.savefig(roc_curve_path)
plt.show()
print(f"Courbe ROC sauvegardée dans {roc_curve_path}")

# Évaluation du modèle
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])

# Afficher les résultats
print(f"Accuracy: {accuracy:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"Specificity: {specificity:.4f}")
print(f"MCC: {mcc:.4f}")
print(f"AUC: {roc_auc:.4f}")
print("Confusion Matrix:")
print(conf_matrix)

# Sauvegarder les résultats
output_path = "/content/drive/MyDrive/stage PFE decision_tree_results_pruned.txt"
with open(output_path, "w") as file:
    file.write(f"Meilleurs hyperparamètres trouvés : {best_params}\n")
    file.write(f"Meilleur ccp_alpha : {best_alpha}\n")
    file.write(f"Accuracy: {accuracy:.4f}\n")
    file.write(f"Recall: {recall:.4f}\n")
    file.write(f"F1-score: {f1:.4f}\n")
    file.write(f"Specificity: {specificity:.4f}\n")
    file.write(f"MCC: {mcc:.4f}\n")
    file.write(f"AUC: {roc_auc:.4f}\n")
    file.write("Confusion Matrix:\n")
    file.write(np.array2string(conf_matrix))

print(f"Résultats sauvegardés dans {output_path}")

# Sauvegarder l'importance des features
feature_importances = pruned_model.feature_importances_
features = X_train.columns
dt_results = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
dt_results = dt_results.sort_values(by='Importance', ascending=False)

# Sauvegarder les importances des features dans un fichier CSV
importances_path = "/content/drive/MyDrive/stage PFE decision_tree_importances_pruned.csv"
dt_results.to_csv(importances_path, index=False)

print(f"Importances des features sauvegardées dans {importances_path}")

"""ce code sans pruning en ajoute le max_depth comme un hyperparamètre qui fixé par le GridSearsh"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix, roc_curve, auc

# Chemins des fichiers train et test
train_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE train_file.csv"
test_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE test_file.csv"

# Charger les fichiers CSV
df_train = pd.read_csv(train_path)
df_test = pd.read_csv(test_path)

# Supprimer la colonne 'nombre of species'
df_train = df_train.drop(columns=['nombre of species'], errors='ignore')
df_test = df_test.drop(columns=['nombre of species'], errors='ignore')

# Séparer les features et les labels
X_train = df_train.drop(columns=['label', 'code de miRNA', 'séquence de miRNA', 'espèce'])
y_train = df_train['label']
X_test = df_test.drop(columns=['label', 'code de miRNA', 'séquence de miRNA', 'espèce'])
y_test = df_test['label']

# Normaliser les données
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Définir les hyperparamètres à tester
param_grid = {
    'max_depth': [ None],
    'min_samples_split': [5],
    'min_samples_leaf': [2]
}

# GridSearchCV pour trouver les meilleurs hyperparamètres
dt = DecisionTreeClassifier(random_state=42)
grid = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy')
grid.fit(X_train_scaled, y_train)

# Meilleurs hyperparamètres trouvés
best_params = grid.best_params_
print(f"Meilleurs hyperparamètres trouvés : {best_params}")

# Entraîner le modèle avec les meilleurs hyperparamètres
model = DecisionTreeClassifier(**best_params, random_state=42)
model.fit(X_train_scaled, y_train)

# Prédictions
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)[:, 1]

# Calcul de l'AUC et courbe ROC
fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

# Tracer la courbe ROC
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Courbe ROC - Decision Tree')
plt.legend(loc='lower right')
roc_curve_path = "/content/drive/MyDrive/stage PFE decision_tree_roc_curve.png"
plt.savefig(roc_curve_path)
plt.show()
print(f"Courbe ROC sauvegardée dans {roc_curve_path}")

# Évaluation du modèle
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])

# Afficher les résultats
print(f"Accuracy: {accuracy:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"Specificity: {specificity:.4f}")
print(f"MCC: {mcc:.4f}")
print(f"AUC: {roc_auc:.4f}")
print("Confusion Matrix:")
print(conf_matrix)

# Sauvegarder les résultats
output_path = "/content/drive/MyDrive/stage PFE decision_tree_results.txt"
with open(output_path, "w") as file:
    file.write(f"Meilleurs hyperparamètres trouvés : {best_params}\n")
    file.write(f"Accuracy: {accuracy:.4f}\n")
    file.write(f"Recall: {recall:.4f}\n")
    file.write(f"F1-score: {f1:.4f}\n")
    file.write(f"Specificity: {specificity:.4f}\n")
    file.write(f"MCC: {mcc:.4f}\n")
    file.write(f"AUC: {roc_auc:.4f}\n")
    file.write("Confusion Matrix:\n")
    file.write(np.array2string(conf_matrix))

print(f"Résultats sauvegardés dans {output_path}")

# Sauvegarder l'importance des features
feature_importances = model.feature_importances_
features = X_train.columns
dt_results = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
dt_results = dt_results.sort_values(by='Importance', ascending=False)

# Sauvegarder les importances des features dans un fichier CSV
importances_path = "/content/drive/MyDrive/stage PFE decision_tree_importances.csv"
dt_results.to_csv(importances_path, index=False)

print(f"Importances des features sauvegardées dans {importances_path}")

"""**RandomForest**

"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix, roc_curve, auc

# Chemins des fichiers train et test
train_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE train_file.csv"
test_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE test_file.csv"

# Charger les fichiers CSV
df_train = pd.read_csv(train_path)
df_test = pd.read_csv(test_path)

# Supprimer la colonne 'nombre of species'
df_train = df_train.drop(columns=['nombre of species'], errors='ignore')
df_test = df_test.drop(columns=['nombre of species'], errors='ignore')

# Séparer les features et les labels
X_train = df_train.drop(columns=['label', 'code de miRNA', 'séquence de miRNA', 'espèce'])
y_train = df_train['label']
X_test = df_test.drop(columns=['label', 'code de miRNA', 'séquence de miRNA', 'espèce'])
y_test = df_test['label']

# Normaliser les données
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Définir les hyperparamètres à tester
param_grid = {
    'n_estimators': [10, 100, 500, 1000],
    'max_depth': [500],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# GridSearchCV pour trouver les meilleurs hyperparamètres
rf = RandomForestClassifier(random_state=42)
grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')
grid.fit(X_train_scaled, y_train)

# Meilleurs hyperparamètres trouvés
best_params = grid.best_params_
print(f"Meilleurs hyperparamètres trouvés : {best_params}")

# Entraîner le modèle avec les meilleurs hyperparamètres
model = RandomForestClassifier(**best_params, random_state=42)
model.fit(X_train_scaled, y_train)

# Prédictions
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)[:, 1]

# Calcul de l'AUC et courbe ROC
fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

# Tracer la courbe ROC
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Courbe ROC - Random Forest')
plt.legend(loc='lower right')
roc_curve_path = "/content/drive/MyDrive/stage PFE random_forest_roc_curve.png"
plt.savefig(roc_curve_path)
plt.show()
print(f"Courbe ROC sauvegardée dans {roc_curve_path}")

# Évaluation du modèle
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])

# Afficher les résultats
print(f"Accuracy: {accuracy:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"Specificity: {specificity:.4f}")
print(f"MCC: {mcc:.4f}")
print(f"AUC: {roc_auc:.4f}")
print("Confusion Matrix:")
print(conf_matrix)

# Sauvegarder les résultats
output_path = "/content/drive/MyDrive/stage PFE random_forest_results.txt"
with open(output_path, "w") as file:
    file.write(f"Meilleurs hyperparamètres trouvés : {best_params}\n")
    file.write(f"Accuracy: {accuracy:.4f}\n")
    file.write(f"Recall: {recall:.4f}\n")
    file.write(f"F1-score: {f1:.4f}\n")
    file.write(f"Specificity: {specificity:.4f}\n")
    file.write(f"MCC: {mcc:.4f}\n")
    file.write(f"AUC: {roc_auc:.4f}\n")
    file.write("Confusion Matrix:\n")
    file.write(np.array2string(conf_matrix))

print(f"Résultats sauvegardés dans {output_path}")

# Sauvegarder l'importance des features
feature_importances = model.feature_importances_
features = X_train.columns
rf_results = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
rf_results = rf_results.sort_values(by='Importance', ascending=False)

# Sauvegarder les importances des features dans un fichier CSV
importances_path = "/content/drive/MyDrive/stage PFE random_forest_importances.csv"
rf_results.to_csv(importances_path, index=False)

print(f"Importances des features sauvegardées dans {importances_path}")

"""**RandomForest avec auters hyperparamètres**"""

# Chemins des fichiers train et test
train_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE train_file.csv"
test_path = "/content/drive/MyDrive/stage PFE ML classifieur /stage PFE test_file.csv"

# Charger les fichiers CSV
df_train = pd.read_csv(train_path)
df_test = pd.read_csv(test_path)

# Supprimer la colonne 'nombre of species'
df_train = df_train.drop(columns=['nombre of species'], errors='ignore')
df_test = df_test.drop(columns=['nombre of species'], errors='ignore')

# Séparer les features et les labels
X_train = df_train.drop(columns=['label', 'code de miRNA', 'séquence de miRNA', 'espèce'])
y_train = df_train['label']
X_test = df_test.drop(columns=['label', 'code de miRNA', 'séquence de miRNA', 'espèce'])
y_test = df_test['label']

# Normaliser les données
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Définir les hyperparamètres à tester
param_grid = {
    'n_estimators': [500],                      # Nombre d'arbres
    'max_depth': [3, 5, 10, None],                         # Profondeur maximale des arbres
    'min_samples_split': [2, 5, 10],                        # Nombre minimum d'échantillons pour splitter un nœud
    'min_samples_leaf': [1, 2, 4],                          # Nombre minimum d'échantillons pour être une feuille
    'max_features': ['auto', 'sqrt', 'log2', None, 0.5],  # Nombre maximal de caractéristiques à utiliser à chaque division
    'bootstrap': [True],                             # Utilisation de l'échantillonnage avec ou sans remplacement
    'criterion': ['gini', 'entropy'],                      # Critère pour évaluer la qualité des divisions
    'oob_score': [True, False],                             # Utilisation des échantillons out-of-bag pour estimer l'erreur
    'class_weight': ['balanced', None]                      # Pondération des classes
}

# GridSearchCV pour trouver les meilleurs hyperparamètres
rf = RandomForestClassifier(random_state=42)
grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)  # n_jobs=-1 pour utiliser tous les cœurs
grid.fit(X_train_scaled, y_train)

# Meilleurs hyperparamètres trouvés
best_params = grid.best_params_
print(f"Meilleurs hyperparamètres trouvés : {best_params}")

# Entraîner le modèle avec les meilleurs hyperparamètres
model = RandomForestClassifier(**best_params, random_state=42)
model.fit(X_train_scaled, y_train)

# Prédictions
y_pred = model.predict(X_test_scaled)

# Évaluation du modèle
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])

# Afficher les résultats
print(f"Accuracy: {accuracy:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"Specificity: {specificity:.4f}")
print(f"MCC: {mcc:.4f}")
print("Confusion Matrix:")
print(conf_matrix)

# Sauvegarder les résultats
output_path = "/content/drive/MyDrive/stage PFE random_forest_results.txt"
with open(output_path, "w") as file:
    file.write(f"Meilleurs hyperparamètres trouvés : {best_params}\n")
    file.write(f"Accuracy: {accuracy:.4f}\n")
    file.write(f"Recall: {recall:.4f}\n")
    file.write(f"F1-score: {f1:.4f}\n")
    file.write(f"Specificity: {specificity:.4f}\n")
    file.write(f"MCC: {mcc:.4f}\n")
    file.write("Confusion Matrix:\n")
    file.write(np.array2string(conf_matrix))

print(f"Résultats sauvegardés dans {output_path}")

# Sauvegarder l'importance des features
feature_importances = model.feature_importances_
features = X_train.columns
rf_results = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
rf_results = rf_results.sort_values(by='Importance', ascending=False)

# Sauvegarder les importances des features dans un fichier CSV
importances_path = "/content/drive/MyDrive/stage PFE random_forest_importances.csv"
rf_results.to_csv(importances_path, index=False)

print(f"Importances des features sauvegardées dans {importances_path}")